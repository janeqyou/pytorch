{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Pierian-Data-Logo.PNG\">\n",
    "<br>\n",
    "<strong><center>Copyright 2019. Created by Jose Marcial Portilla.</center></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with PyTorch\n",
    "In this section we'll use PyTorch's machine learning model to progressively develop a best-fit line for a given set of data points. Like most linear regression algorithms, we're seeking to minimize the error between our model and the actual data, using a <em>loss function</em> like mean-squared-error.\n",
    "\n",
    "<img src='../Images/linear-regression-residuals.png' width='400' style=\"display: inline-block\"><br>\n",
    "\n",
    "Image source: <a href='https://commons.wikimedia.org/wiki/File:Residuals_for_Linear_Regression_Fit.png'>https://commons.wikimedia.org/wiki/File:Residuals_for_Linear_Regression_Fit.png</a>\n",
    "\n",
    "To start, we'll develop a collection of data points that appear random, but that fit a known linear equation $y = 2x+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-400d09a59cdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # we'll use this a lot going forward!\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a column matrix of X values\n",
    "We can create tensors right away rather than convert from NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\ntorch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "X = torch.linspace(1,50,50).reshape(-1,1)\n",
    "x_intercept = X[1:10,0]\n",
    "print (x_intercept)\n",
    "print(X.size())\n",
    "# Equivalent to\n",
    "# X = torch.unsqueeze(torch.linspace(1,50,50), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a \"random\" array of error values\n",
    "We want 50 random integer values that collectively cancel each other out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.)\ntorch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(71) # to obtain reproducible results\n",
    "e = torch.randint(-8,9,(50,1),dtype=torch.float) # low, high, size\n",
    "print(e.sum())\n",
    "print(e.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a column matrix of y values\n",
    "Here we'll set our own parameters of $\\mathrm {weight} = 2,\\; \\mathrm {bias} = 1$, plus the error amount.<br><strong><tt>y</tt></strong> will have the same shape as <strong><tt>X</tt></strong> and <strong><tt>e</tt></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "y = 2*X + 1 + e\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results\n",
    "We have to convert tensors to NumPy arrays just for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-97a2540aa142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(X.numpy(), y.numpy())\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we created tensor $X$, we did <em>not</em> pass <tt>requires_grad=True</tt>. This means that $y$ doesn't have a gradient function, and <tt>y.backward()</tt> won't work. Since PyTorch is not tracking operations, it doesn't know the relationship between $X$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear model\n",
    "As a quick demonstration we'll show how the built-in <tt>nn.Linear()</tt> model preselects weight and bias values at random.\n",
    "\n",
    "torch.nn.Linear(in_features: int, out_features: int, bias: bool = True)\n",
    " - in_features: size of input \n",
    " - out_features: size of output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[0.1060]], requires_grad=True)\nParameter containing:\ntensor([0.9638], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(59)\n",
    "\n",
    "model = nn.Linear(in_features=1, out_features=1)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without seeing any data, the model sets a random weight of 0.1060 and a bias of 0.9638."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model classes\n",
    "PyTorch lets us define models as object classes that can store multiple model layers. In upcoming sections we'll set up several neural network layers, and determine how each layer should perform its forward pass to the next layer. For now, though, we only need a single <tt>linear</tt> layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>NOTE:</strong> The \"Linear\" model layer used here doesn't really refer to linear regression. Instead, it describes the type of neural network layer employed. Linear layers are also called \"fully connected\" or \"dense\" layers. Going forward our models may contain linear layers, convolutional layers, and more.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When <tt>Model</tt> is instantiated, we need to pass in the size (dimensions) of the incoming and outgoing features. For our purposes we'll use (1,1).<br>As above, we can see the initial hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model(\n  (linear): Linear(in_features=1, out_features=1, bias=True)\n)\nWeight: 0.10597813129425049\nBias:   0.9637961387634277\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(59)\n",
    "model = Model(1, 1)\n",
    "print(model)\n",
    "print('Weight:', model.linear.weight.item())\n",
    "print('Bias:  ', model.linear.bias.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As models become more complex, it may be better to iterate over all the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "linear.weight \t 0.10597813129425049\nlinear.bias \t 0.9637961387634277\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, '\\t', param.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>NOTE:</strong> In the above example we had our Model class accept arguments for the number of input and output features.<br>For simplicity we can hardcode them into the Model:\n",
    "         \n",
    "<tt><font color=black>\n",
    "class Model(torch.nn.Module):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;def \\_\\_init\\_\\_(self):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().\\_\\_init\\_\\_()<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.linear = Linear(1,1)<br><br>\n",
    "model = Model()\n",
    "</font></tt><br><br>\n",
    "\n",
    "Alternatively we can use default arguments:\n",
    "\n",
    "<tt><font color=black>\n",
    "class Model(torch.nn.Module):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;def \\_\\_init\\_\\_(self, in_dim=1, out_dim=1):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().\\_\\_init\\_\\_()<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.linear = Linear(in_dim,out_dim)<br><br>\n",
    "model = Model()<br>\n",
    "<em>\\# or</em><br>\n",
    "model = Model(i,o)</font></tt>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the result when we pass a tensor into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.1758], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0])\n",
    "print(model.forward(x))   # equivalent to print(model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is confirmed with $f(x) = (0.1060)(2.0)+(0.9638) = 1.1758$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the initial model\n",
    "We can plot the untrained model against our dataset to get an idea of our starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 1. 50.]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([X.min(),X.max()])\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial weight: 0.10597813, Initial bias: 0.96379614\n\n[1.0697743 6.2627025]\n"
     ]
    }
   ],
   "source": [
    "w1,b1 = model.linear.weight.item(), model.linear.bias.item()\n",
    "print(f'Initial weight: {w1:.8f}, Initial bias: {b1:.8f}')\n",
    "print()\n",
    "\n",
    "y1 = x1*w1 + b1\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7f9015d63bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initial Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(X.numpy(), y.numpy())\n",
    "plt.plot(x1,y1,'r')\n",
    "plt.title('Initial Model')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the loss function\n",
    "We could write our own function to apply a Mean Squared Error (MSE) that follows<br>\n",
    "\n",
    "$\\begin{split}MSE &= \\frac {1} {n} \\sum_{i=1}^n {(y_i - \\hat y_i)}^2 \\\\\n",
    "&= \\frac {1} {n} \\sum_{i=1}^n {(y_i - (wx_i + b))}^2\\end{split}$<br>\n",
    "\n",
    "Fortunately PyTorch has it built in.<br>\n",
    "<em>By convention, you'll see the variable name \"criterion\" used, but feel free to use something like \"linear_loss_func\" if that's clearer.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the optimization\n",
    "Here we'll use <a href='https://en.wikipedia.org/wiki/Stochastic_gradient_descent'>Stochastic Gradient Descent</a> (SGD) with an applied <a href='https://en.wikipedia.org/wiki/Learning_rate'>learning rate</a> (lr) of 0.001. Recall that the learning rate tells the optimizer how much to adjust each parameter on the next round of calculations. Too large a step and we run the risk of overshooting the minimum, causing the algorithm to diverge. Too small and it will take a long time to converge.\n",
    "\n",
    "For more complicated (multivariate) data, you might also consider passing optional <a href='https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum'><tt>momentum</tt></a> and <a href='https://en.wikipedia.org/wiki/Tikhonov_regularization'><tt>weight_decay</tt></a> arguments. Momentum allows the algorithm to \"roll over\" small bumps to avoid local minima that can cause convergence too soon. Weight decay (also called an L2 penalty) applies to biases.\n",
    "\n",
    "For more information, see <a href='https://pytorch.org/docs/stable/optim.html'><strong><tt>torch.optim</tt></strong></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "# You'll sometimes see this as\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "An <em>epoch</em> is a single pass through the entire dataset. We want to pick a sufficiently large number of epochs to reach a plateau close to our known parameters of $\\mathrm {weight} = 2,\\; \\mathrm {bias} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>Let's walk through the steps we're about to take:</strong><br>\n",
    "\n",
    "1. Set a reasonably large number of passes<br>\n",
    "<tt><font color=black>epochs = 50</font></tt><br>\n",
    "2. Create a list to store loss values. This will let us view our progress afterward.<br>\n",
    "<tt><font color=black>losses = []</font></tt><br>\n",
    "<tt><font color=black>for i in range(epochs):</font></tt><br>\n",
    "3. Bump \"i\" so that the printed report starts at 1<br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;i+=1</font></tt><br>\n",
    "4. Create a prediction set by running \"X\" through the current model parameters<br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;y_pred = model.forward(X)</font></tt><br>\n",
    "5. Calculate the loss<br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;loss = criterion(y_pred, y)</font></tt><br>\n",
    "6. Add the loss value to our tracking list<br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;losses.append(loss)</font></tt><br>\n",
    "7. Print the current line of results<br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;print(f'epoch: {i:2} loss: {loss.item():10.8f}')</font></tt><br>\n",
    "8. Gradients accumulate with every backprop. To prevent compounding we need to reset the stored gradient for each new epoch.<br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zero_grad()</font></tt><br>\n",
    "9. Now we can backprop<br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;loss.backward()</font></tt><br>\n",
    "10. Finally, we can update the hyperparameters of our model<br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step()</font></tt>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  1  loss: 3057.21679688  weight: 0.10597813  bias: 0.96379614\nepoch:  2  loss: 1588.53088379  weight: 3.33490038  bias: 1.06046367\nepoch:  3  loss: 830.30017090  weight: 1.01483274  bias: 0.99226278\nepoch:  4  loss: 438.85241699  weight: 2.68179965  bias: 1.04252183\nepoch:  5  loss: 236.76152039  weight: 1.48402119  bias: 1.00766504\nepoch:  6  loss: 132.42912292  weight: 2.34460592  bias: 1.03396463\nepoch:  7  loss: 78.56573486  weight: 1.72622538  bias: 1.01632178\nepoch:  8  loss: 50.75775909  weight: 2.17050409  bias: 1.03025162\nepoch:  9  loss: 36.40123367  weight: 1.85124576  bias: 1.02149546\nepoch: 10  loss: 28.98923111  weight: 2.08060074  bias: 1.02903891\nepoch: 11  loss: 25.16238785  weight: 1.91576838  bias: 1.02487016\nepoch: 12  loss: 23.18647385  weight: 2.03416562  bias: 1.02911627\nepoch: 13  loss: 22.16612244  weight: 1.94905841  bias: 1.02731562\nepoch: 14  loss: 21.63911057  weight: 2.01017213  bias: 1.02985907\nepoch: 15  loss: 21.36676979  weight: 1.96622372  bias: 1.02928054\nepoch: 16  loss: 21.22591972  weight: 1.99776423  bias: 1.03094459\nepoch: 17  loss: 21.15294456  weight: 1.97506487  bias: 1.03099668\nepoch: 18  loss: 21.11501312  weight: 1.99133754  bias: 1.03220642\nepoch: 19  loss: 21.09518051  weight: 1.97960854  bias: 1.03258383\nepoch: 20  loss: 21.08468437  weight: 1.98799884  bias: 1.03355861\nepoch: 21  loss: 21.07901382  weight: 1.98193336  bias: 1.03410351\nepoch: 22  loss: 21.07583046  weight: 1.98625445  bias: 1.03495669\nepoch: 23  loss: 21.07394028  weight: 1.98311269  bias: 1.03558779\nepoch: 24  loss: 21.07270241  weight: 1.98533309  bias: 1.03637791\nepoch: 25  loss: 21.07181931  weight: 1.98370099  bias: 1.03705311\nepoch: 26  loss: 21.07110596  weight: 1.98483658  bias: 1.03781021\nepoch: 27  loss: 21.07048416  weight: 1.98398376  bias: 1.03850794\nepoch: 28  loss: 21.06991386  weight: 1.98455977  bias: 1.03924775\nepoch: 29  loss: 21.06937027  weight: 1.98410904  bias: 1.03995669\nepoch: 30  loss: 21.06883812  weight: 1.98439610  bias: 1.04068720\nepoch: 31  loss: 21.06830788  weight: 1.98415291  bias: 1.04140162\nepoch: 32  loss: 21.06778145  weight: 1.98429084  bias: 1.04212701\nepoch: 33  loss: 21.06726646  weight: 1.98415494  bias: 1.04284394\nepoch: 34  loss: 21.06674004  weight: 1.98421574  bias: 1.04356635\nepoch: 35  loss: 21.06622505  weight: 1.98413551  bias: 1.04428422\nepoch: 36  loss: 21.06570625  weight: 1.98415649  bias: 1.04500473\nepoch: 37  loss: 21.06518555  weight: 1.98410451  bias: 1.04572272\nepoch: 38  loss: 21.06467056  weight: 1.98410523  bias: 1.04644191\nepoch: 39  loss: 21.06415749  weight: 1.98406804  bias: 1.04715967\nepoch: 40  loss: 21.06364059  weight: 1.98405814  bias: 1.04787791\nepoch: 41  loss: 21.06312180  weight: 1.98402870  bias: 1.04859519\nepoch: 42  loss: 21.06260490  weight: 1.98401320  bias: 1.04931259\nepoch: 43  loss: 21.06209564  weight: 1.98398757  bias: 1.05002928\nepoch: 44  loss: 21.06157494  weight: 1.98396957  bias: 1.05074584\nepoch: 45  loss: 21.06107140  weight: 1.98394585  bias: 1.05146194\nepoch: 46  loss: 21.06055450  weight: 1.98392630  bias: 1.05217779\nepoch: 47  loss: 21.06004333  weight: 1.98390377  bias: 1.05289316\nepoch: 48  loss: 21.05953407  weight: 1.98388338  bias: 1.05360830\nepoch: 49  loss: 21.05901527  weight: 1.98386145  bias: 1.05432308\nepoch: 50  loss: 21.05850792  weight: 1.98384094  bias: 1.05503750\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i+=1\n",
    "    y_pred = model.forward(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    losses.append(loss)\n",
    "    print(f'epoch: {i:2}  loss: {loss.item():10.8f}  weight: {model.linear.weight.item():10.8f}  \\\n",
    "bias: {model.linear.bias.item():10.8f}') \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss values\n",
    "Let's see how loss changed over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5010c8700017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(range(epochs), losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the result\n",
    "Now we'll derive <tt>y1</tt> from the new model to plot the most recent best-fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Current weight: 1.98381913, Current bias: 1.05575156\n\n[ 1. 50.]\n[  3.0395708 100.246704 ]\n"
     ]
    }
   ],
   "source": [
    "w1,b1 = model.linear.weight.item(), model.linear.bias.item()\n",
    "print(f'Current weight: {w1:.8f}, Current bias: {b1:.8f}')\n",
    "print()\n",
    "\n",
    "y1 = x1*w1 + b1\n",
    "print(x1)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d3ec4d3a69a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Current Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(X.numpy(), y.numpy())\n",
    "plt.plot(x1,y1,'r')\n",
    "plt.title('Current Model')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('pytorchenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2dad65605baca0d241fdd9509531bf4058ddbda272ab5d82212efa73b2253b81"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}